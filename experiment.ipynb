{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be16f17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from architectures.gru_seq2seq_bidirectional_enc import Builder as AudioVAEBuilder\n",
    "from architectures.gru_seq2seq_bidirectional_enc import Wrapper as AudioVAEWrapper\n",
    "\n",
    "from readers import AudioReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7f60e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.audio import (\n",
    "    concat_FT,\n",
    "    reverse_FT,\n",
    "    get_waveform_from_spectrogram_tensor,\n",
    "    play_audio\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65b9fea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch\n",
    "from scipy import signal\n",
    "import sounddevice as sd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd07e7f",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d108e43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = AudioVAEBuilder()\n",
    "audio_model = builder.build(\n",
    "    embedding_dim=2050,\n",
    "    latent_dim=128,\n",
    "    context_length=944,\n",
    "    num_layers=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "730fa2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_wrapper = AudioVAEWrapper(audio_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4378ffc7",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f11ddece",
   "metadata": {},
   "outputs": [],
   "source": [
    "fourier_params = {\n",
    "    'fs': 16000,\n",
    "    'window_size': 2048,\n",
    "    'window_shift': 1024,\n",
    "    'type': \"hamming\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "675623d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = AudioReader(fourier_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfdcc51f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 944, 1025])\n"
     ]
    }
   ],
   "source": [
    "for audio in dataset:\n",
    "    print(audio.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "182f3dcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageVAE(\n",
       "  (encoder): BidirectionalEncoder(\n",
       "    (encoder): GRU(2050, 2050, batch_first=True, bidirectional=True)\n",
       "    (mu_proj): Sequential(\n",
       "      (0): Linear(in_features=4100, out_features=128, bias=True)\n",
       "    )\n",
       "    (sigma_proj): Sequential(\n",
       "      (0): Linear(in_features=4100, out_features=128, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (decoder): AutoregressiveDecoder(\n",
       "    (proj_h): Linear(in_features=128, out_features=2050, bias=True)\n",
       "    (decoder): GRU(2050, 2050, batch_first=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "914bcc3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 944, 2050]) torch.Size([1, 944, 2050])\n"
     ]
    }
   ],
   "source": [
    "for audio in dataset:\n",
    "    X = concat_FT(audio).cuda()\n",
    "    output = audio_wrapper(X)\n",
    "    print(X.shape, output[0].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df343688",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05d32537",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, 32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a6a25d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(audio_wrapper.parameters(), lr=1e-3, betas=(0.5, 0.999), weight_decay=1e-5)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2771dbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ac76481",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageVAE(\n",
       "  (encoder): BidirectionalEncoder(\n",
       "    (encoder): GRU(2050, 2050, batch_first=True, bidirectional=True)\n",
       "    (mu_proj): Sequential(\n",
       "      (0): Linear(in_features=4100, out_features=128, bias=True)\n",
       "    )\n",
       "    (sigma_proj): Sequential(\n",
       "      (0): Linear(in_features=4100, out_features=128, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (decoder): AutoregressiveDecoder(\n",
       "    (proj_h): Linear(in_features=128, out_features=2050, bias=True)\n",
       "    (decoder): GRU(2050, 2050, batch_first=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2437bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 4.588586220734214e-05\n",
      "Epoch: 1, Loss: 9.217205045075616e-07\n",
      "Epoch: 2, Loss: 2.8083185218408244e-07\n",
      "Epoch: 3, Loss: 2.7794322525753757e-07\n",
      "Epoch: 4, Loss: 2.7914018762231763e-07\n",
      "Epoch: 5, Loss: 2.8066664911507997e-07\n",
      "Epoch: 6, Loss: 2.8208117858863346e-07\n",
      "Epoch: 7, Loss: 2.8364625015875333e-07\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for audio in dataset:\n",
    "        optimizer.zero_grad()\n",
    "        audio = concat_FT(audio).cuda()\n",
    "\n",
    "        audio_pred, _, _ = audio_wrapper(audio, audio, teacher_forcing=0.5)\n",
    "        loss = criterion(audio_pred, audio)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch: {epoch}, Loss: {total_loss/len(dataset)}')\n",
    "    total_loss = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6257a32f",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4301dfe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageVAE(\n",
       "  (encoder): BidirectionalEncoder(\n",
       "    (encoder): GRU(2050, 2050, batch_first=True, bidirectional=True)\n",
       "    (mu_proj): Sequential(\n",
       "      (0): Linear(in_features=4100, out_features=128, bias=True)\n",
       "    )\n",
       "    (sigma_proj): Sequential(\n",
       "      (0): Linear(in_features=4100, out_features=128, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (decoder): AutoregressiveDecoder(\n",
       "    (proj_h): Linear(in_features=128, out_features=2050, bias=True)\n",
       "    (decoder): GRU(2050, 2050, batch_first=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a53e9a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "for wave in dataset:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6abac685",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = get_waveform_from_spectrogram_tensor(wave.cpu(), fourier_params)\n",
    "play_audio(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c0cb2abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "wave_recon = audio_model(concat_FT(wave).cuda())[0]\n",
    "audio_recon = reverse_FT(wave_recon)\n",
    "audio_recon = get_waveform_from_spectrogram_tensor(audio_recon.detach().cpu(), fourier_params)\n",
    "play_audio(audio_recon)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xlstm_up",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
