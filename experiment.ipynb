{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be16f17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from architectures.linear_attributes_autoencoder import Builder as AttrVAEBuilder\n",
    "\n",
    "from architectures.gru_seq2seq_bidirectional_enc import Builder as AudioVAEBuilder\n",
    "from architectures.gru_seq2seq_bidirectional_enc import Wrapper as AudioVAEWrapper\n",
    "\n",
    "from readers import AudioReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65b9fea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd07e7f",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4df50562",
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_vae = AttrVAEBuilder().build(\n",
    "    128,\n",
    "    [32, 128, 512]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d573616",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageVAE(\n",
       "  (encoder): LinearEncoder(\n",
       "    (layers): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=128, bias=True)\n",
       "        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (mu_proj): Linear(in_features=512, out_features=128, bias=True)\n",
       "    (logvar_proj): Linear(in_features=512, out_features=128, bias=True)\n",
       "  )\n",
       "  (decoder): LinearDecoder(\n",
       "    (latent_proj): Linear(in_features=128, out_features=512, bias=True)\n",
       "    (decoder_blocks): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=32, bias=True)\n",
       "        (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attr_vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d108e43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = AudioVAEBuilder()\n",
    "audio_model = builder.build(\n",
    "    embedding_dim=2050,\n",
    "    latent_dim=128,\n",
    "    context_length=944,\n",
    "    num_layers=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "730fa2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_wrapper = AudioVAEWrapper(audio_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4378ffc7",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ea16cd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.audio import (\n",
    "    concat_FT,\n",
    "    reverse_FT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f11ddece",
   "metadata": {},
   "outputs": [],
   "source": [
    "fourier_params = {\n",
    "    'fs': 16000,\n",
    "    'window_size': 2048,\n",
    "    'window_shift': 1024,\n",
    "    'type': \"hamming\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "675623d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = AudioReader(fourier_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfdcc51f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 944, 1025])\n"
     ]
    }
   ],
   "source": [
    "for audio in dataset:\n",
    "    print(audio.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "182f3dcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageVAE(\n",
       "  (encoder): BidirectionalEncoder(\n",
       "    (encoder): GRU(2050, 2050, batch_first=True, bidirectional=True)\n",
       "    (mu_proj): Sequential(\n",
       "      (0): Linear(in_features=4100, out_features=128, bias=True)\n",
       "    )\n",
       "    (sigma_proj): Sequential(\n",
       "      (0): Linear(in_features=4100, out_features=128, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (decoder): AutoregressiveDecoder(\n",
       "    (proj_h): Linear(in_features=128, out_features=2050, bias=True)\n",
       "    (decoder): GRU(2050, 2050, batch_first=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "914bcc3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 944, 2050]) torch.Size([1, 944, 2050])\n"
     ]
    }
   ],
   "source": [
    "for audio in dataset:\n",
    "    X = concat_FT(audio).cuda()\n",
    "    output = audio_wrapper(X)\n",
    "    print(X.shape, output[0].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df343688",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a6a25d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(audio_wrapper.parameters(), lr=1e-3, betas=(0.5, 0.999), weight_decay=1e-5)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2771dbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6ac76481",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageVAE(\n",
       "  (encoder): BidirectionalEncoder(\n",
       "    (encoder): GRU(2050, 2050, batch_first=True, bidirectional=True)\n",
       "    (mu_proj): Sequential(\n",
       "      (0): Linear(in_features=4100, out_features=128, bias=True)\n",
       "    )\n",
       "    (sigma_proj): Sequential(\n",
       "      (0): Linear(in_features=4100, out_features=128, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (decoder): AutoregressiveDecoder(\n",
       "    (proj_h): Linear(in_features=128, out_features=2050, bias=True)\n",
       "    (decoder): GRU(2050, 2050, batch_first=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0115b17d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 2.0337943602726228e-05\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m optimizer.zero_grad()\n\u001b[32m      5\u001b[39m audio = concat_FT(audio).cuda()\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m audio_pred, _, _ = \u001b[43maudio_wrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m loss = criterion(audio_pred, audio)\n\u001b[32m      9\u001b[39m total_loss += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ricardo\\miniconda3\\envs\\xlstm_up\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ricardo\\miniconda3\\envs\\xlstm_up\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ricardo\\tfg\\taVAE\\architectures\\gru_seq2seq_bidirectional_enc\\wrapper.py:22\u001b[39m, in \u001b[36mWrapperGRU.forward\u001b[39m\u001b[34m(self, x, target, teacher_forcing)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor,\n\u001b[32m     19\u001b[39m             target: Optional[Tensor] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     20\u001b[39m             teacher_forcing: Optional[\u001b[38;5;28mint\u001b[39m] = \u001b[32m0\u001b[39m) -> Tensor:\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     mu, logvar = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m     z = \u001b[38;5;28mself\u001b[39m.model.reparametrize(mu, logvar)\n\u001b[32m     25\u001b[39m     h = \u001b[38;5;28mself\u001b[39m.model.decoder.proj_h(z).unsqueeze(\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ricardo\\tfg\\taVAE\\framework\\architectures\\image_variational_autoencoder.py:24\u001b[39m, in \u001b[36mImageVAE.encode\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) -> Tuple[Mu, Sigma]:\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ricardo\\miniconda3\\envs\\xlstm_up\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ricardo\\miniconda3\\envs\\xlstm_up\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ricardo\\tfg\\taVAE\\architectures\\gru_seq2seq_bidirectional_enc\\encoder.py:36\u001b[39m, in \u001b[36mBidirectionalEncoder.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) -> Tuple[Mu, Sigma]:\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m     _, h = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m     h = torch.cat([h[-\u001b[32m2\u001b[39m], h[-\u001b[32m1\u001b[39m]], dim=-\u001b[32m1\u001b[39m)\n\u001b[32m     39\u001b[39m     mu = \u001b[38;5;28mself\u001b[39m.mu_proj(h)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ricardo\\miniconda3\\envs\\xlstm_up\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ricardo\\miniconda3\\envs\\xlstm_up\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ricardo\\miniconda3\\envs\\xlstm_up\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:1393\u001b[39m, in \u001b[36mGRU.forward\u001b[39m\u001b[34m(self, input, hx)\u001b[39m\n\u001b[32m   1391\u001b[39m \u001b[38;5;28mself\u001b[39m.check_forward_args(\u001b[38;5;28minput\u001b[39m, hx, batch_sizes)\n\u001b[32m   1392\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1393\u001b[39m     result = \u001b[43m_VF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgru\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1394\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1395\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1396\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1397\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1398\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1399\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1400\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1401\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1402\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbatch_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1403\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1404\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1405\u001b[39m     result = _VF.gru(\n\u001b[32m   1406\u001b[39m         \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   1407\u001b[39m         batch_sizes,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1414\u001b[39m         \u001b[38;5;28mself\u001b[39m.bidirectional,\n\u001b[32m   1415\u001b[39m     )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for audio in dataset:\n",
    "        optimizer.zero_grad()\n",
    "        audio = concat_FT(audio).cuda()\n",
    "\n",
    "        audio_pred, _, _ = audio_wrapper(audio)\n",
    "        loss = criterion(audio_pred, audio)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch: {epoch}, Loss: {total_loss/len(dataset)}')\n",
    "    total_loss = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6257a32f",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4301dfe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageVAE(\n",
       "  (encoder): BidirectionalEncoder(\n",
       "    (encoder): GRU(2050, 2050, batch_first=True, bidirectional=True)\n",
       "    (mu_proj): Sequential(\n",
       "      (0): Linear(in_features=4100, out_features=128, bias=True)\n",
       "    )\n",
       "    (sigma_proj): Sequential(\n",
       "      (0): Linear(in_features=4100, out_features=128, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (decoder): AutoregressiveDecoder(\n",
       "    (proj_h): Linear(in_features=128, out_features=2050, bias=True)\n",
       "    (decoder): GRU(2050, 2050, batch_first=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d0f1cca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "\n",
    "def get_waveform_from_spectrogram_tensor(X: torch.Tensor, stft_params: dict) -> np.ndarray:\n",
    "    X = X.squeeze(0).permute(1, 0)\n",
    "    X_np = X.numpy()\n",
    "\n",
    "    _, waveform = signal.istft(X_np,\n",
    "                               fs=stft_params['fs'],\n",
    "                               nperseg=stft_params['window_size'],\n",
    "                               noverlap=stft_params['window_size'] - stft_params['window_shift'],\n",
    "                               window=stft_params['type'])\n",
    "    \n",
    "    return waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c16d5484",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def play_audio(waveform, sample_rate=16000, duration=5):\n",
    "    if isinstance(waveform, torch.Tensor):\n",
    "        waveform = waveform.detach().cpu().numpy()\n",
    "    \n",
    "    if waveform.ndim > 1:\n",
    "        waveform = np.squeeze(waveform)\n",
    "    \n",
    "    max_samples = sample_rate * duration\n",
    "    waveform = waveform[:max_samples]\n",
    "\n",
    "    waveform = waveform.astype(np.float32)\n",
    "\n",
    "    sd.play(waveform, samplerate=sample_rate)\n",
    "    sd.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a53e9a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "for wave in dataset:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6abac685",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = get_waveform_from_spectrogram_tensor(wave.cpu(), fourier_params)\n",
    "play_audio(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c0cb2abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "wave_recon = audio_model(concat_FT(wave).cuda())[0]\n",
    "audio_recon = reverse_FT(wave_recon)\n",
    "audio_recon = get_waveform_from_spectrogram_tensor(audio_recon.detach().cpu(), fourier_params)\n",
    "play_audio(audio_recon)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xlstm_up",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
